---
title: "Package Delay Report"
author: "Constance Streitman"
format:
  html:
    output-file: "index.html"
editor: visual
---

```{r}
#| include: false
#libraries
library(multcomp) 
library(MASS)     

#tidyverse AFTER MASS to mask MASS::select
library(tidyverse) 

library(caret)
library(randomForest)
library(pROC)
library(FactoMineR)
library(broom)

df <- read.csv("logistics_shipments_dataset.CSV")
```

## Introduction

I chose the dataset off of Kaggle, published in August by Shahriar Kabir and titled "[US Logistics Performance Dataset: Simulated Shipment Data for Cost Reduction & Delivery Reliability](https://www.kaggle.com/datasets/shahriarkabir/us-logistics-performance-dataset)". This was for a class project, and I browsed through Kaggle simply looking for something business-related that had an easy problem to answer. After seeing the "Status" column for this one, I knew this would be the one.

Thus, I set out to answer the question of inferring what variables affect package status, or in other words, what factors indicate a package will end up delayed or lost instead of it arriving on-time.

## Data, cleaning & pre-processing

At first, there were 2,000 samples and 9 predictors. 3 were categorical, 3 were numeric, and 3 were in a strange limbo of uselessness. The three limbo values were the shipment ID, the shipment date, and the delivery date. I removed the shipment ID, and a peer used the shipment date and delivery date to find the amount of days spent in transit (need to include in report, but shall hide).

```{r}
df <- subset(df, select = -c(Shipment_ID))
```

```{r}
#| include: false
df$Shipment_Date <- as.Date(df$Shipment_Date)
df$Delivery_Date <- as.Date(df$Delivery_Date)

error_rows <- which(df$Shipment_Date > df$Delivery_Date)
temp_ship_date <- df$Shipment_Date[error_rows]
df$Shipment_Date[error_rows] <- df$Delivery_Date[error_rows]
df$Delivery_Date[error_rows] <- temp_ship_date

df$Transit_Days <- as.numeric(df$Delivery_Date - df$Shipment_Date)
df$Transit_Days[is.na(df$Transit_Days)] <- median(df$Transit_Days, na.rm = TRUE)
#making everything below a factor bc idk ur supposed to
df$Origin_Warehouse = as.factor(df$Origin_Warehouse)
df$Destination = as.factor(df$Destination)
df$Carrier = as.factor(df$Carrier)
#df$D_or_L = as.factor(df$D_or_L)
# i ran a shapiro-wilk test - Cost is not normal, pval small - so, median
df$Delivery_Date = ifelse(
  is.na(df$Delivery_Date) & !is.na(df$Transit_Days),
  df$Shipment_Date + df$Transit_Days,
  df$Delivery_Date
) #filling in delivery date values where possible, given in transit var exists
```

Also, we removed some samples from the data set, namely, packages that were "In Transit" as opposed to delayed, delivered, or lost. Their final outcome had yet to be known, so it was overall useless for inference in this matter. Then, I made a binary between "Delayed or Lost" and "Delivered" to use later for logistic regression and to make the confusion matrix simpler.

```{r}
df = subset(df, tolower(Status) != "in transit") #deleting "in transit" samples
df$D_or_L <- ifelse(tolower(df$Status) %in% c("delayed", "lost"), 1, 0)
# ^ makes the binary
df <- subset(df, select = -Status)
```

Next, I fixed the issue of there being N/A values present under the cost predictor. I planned to just replace it with the mean or median, but I couldn't even tell what I was supposed to do due to the extreme outliers skewing the visibility of the cost histogram. So, I removed the outliers and tested cost for normality before I could proceed.

```{r}
plot(df$Cost)
Q1 <- quantile(df$Cost, 0.25, na.rm = TRUE)
Q3 <- quantile(df$Cost, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower <- Q1 - 1.5 * IQR
upper <- Q3 + 1.5 * IQR
df_no_outliers <- subset(df, Cost >= lower & Cost <= upper)
hist(df_no_outliers$Cost)
shapiro.test(df_no_outliers$Cost)

df$Cost[is.na(df$Cost)] = median(df$Cost, na.rm = TRUE)
```

Seeing that the data was non-normal, I replaced the N/A values with the Cost median.

Then, I added the Weekday value, which took the date of shipment and said what day of the week it was. I was curious if packages shipped on Fridays or Mondays had different outcomes, and wanted to test it.

```{r}
df$Delivery_Date <- as.Date(df$Delivery_Date, origin = "1970-01-01")
df$Shipment_Date <- as.Date(df$Shipment_Date, origin = "1970-01-01")
df$ShipWeekday <- factor(
  weekdays(df$Shipment_Date, abbreviate = FALSE),
  levels = c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"),
  ordered = FALSE
)
```

Then, I reordered the columns to be more contiguous—"Delayed or Lost" in the first column, with categorical variables all in one spot, then the dates, and then all the numeric variables.

```{r}
df <- df[, c("D_or_L", "Origin_Warehouse", "Destination", 
             "Carrier", "ShipWeekday", "Shipment_Date", "Delivery_Date",
             "Transit_Days", "Distance_miles", "Weight_kg", "Cost")]
```

## Principal Component Analysis (PCA)

I separated out the numeric components, and then ran PCA to see if I could reduce dimensionality somehow, or just to see what was there. This showed that Transit Days, Cost, and Distance were all part of the first dimension, which represented 41% of the variability of the numeric-only model, followed by Weight, which represented another 25%.

```{r}
dfnumeric = df[8:11]
pc_result = prcomp(dfnumeric, scale. = TRUE)
PCA(dfnumeric, scale= TRUE)
```

The PCA was just to look around, though, not to find anything significant.

## Logistic Regression

So, first I made a logistic regression of the categorical variables only, to see if anything came out significant.

```{r}
df$Shipment_Date <- as.Date(df$Shipment_Date, format = "%Y-%m-%d") #idk

##########################################################
####LOGISIC REGRESSION
CatReg <- glm(D_or_L ~ Origin_Warehouse + Destination + Carrier + ShipWeekday,
             data = df, family = binomial)
summary(CatReg) #NOTE: WArehouse_NYC and Boston have pval low. Also UPS. but
```

The NYC warehouse, Boston as a destination, and the carrier UPS all seemed significant, but with a model this crowded it was strange and hard to tell just what was going on. To confirm, I ran some tests below (with some pre-processing to match), but was still left confused.

```{r}
uglydf = df #so df isnt polluted by bs columns
# predicted probabilities
preds <- augment(CatReg, type.predict = "response", newdata = df)
uglydf$pred_prob <- preds$.fitted

# average predicted prob by destination
uglydf %>%
  group_by(Destination) %>%
  summarise(mean_delay_prob = mean(pred_prob))
#### HOW LIKELY IT IS TO GET DELAY/LOST IN EACH DESTINATION LOCATION!!!
#low is 8%, high is 20% (Boston)
# average predicted prob by WAREHOUSE!!!!!!!!!!
uglydf %>%
  group_by(Origin_Warehouse) %>%
  summarise(mean_delay_prob = mean(pred_prob))
### low of 8% high of 16%

#if this does nothing, its bc it takes forever to load
#basically inconclusive, high p values. get rid of it? idk
```

Then I changed course for a bit and tried out logistic regression using only the numeric predictors. As expected, Transit Days was highly significant, but I had issues with that (more later). I also found that Distance was significant, but to less of a degree.

```{r}
NumReg <- glm(D_or_L ~ Weight_kg + Distance_miles + Cost + Transit_Days,
                 data = df, family = binomial)
summary(NumReg) #pval low for transit days(duh) and for distance_miles
```

Still wondering what was up with UPS, Boston, and NYC, I put the predictors together to see if my answer this time was different.

```{r}
AllReg <- glm(D_or_L ~ Weight_kg + Distance_miles + Cost + Transit_Days +
                           Origin_Warehouse + Destination + Carrier + ShipWeekday,
                         data = df, family = binomial)
summary(AllReg)
```

Curiously, FedEx and OnTrac, two carriers, became significant, but all three other predictors lost their significance with this model. That puzzled me, so I sought to get to the bottom of it with various flags. I included the code, but needless to say, it was a bust, and I gave up trying to use specific carriers and warehouses as anything significant in terms of model impact.

```{r, results='hide'}
uglydf$UPS_flag <- ifelse(df$Carrier == "UPS", 1, 0)
uglydf$NYC_flag <- ifelse(df$Origin_Warehouse == "Warehouse_NYC", 1, 0)
uglydf$Boston_flag = ifelse(df$Destination == "Boston", 1, 0)
uglydf$Tuesday_flag = ifelse(df$ShipWeekday == "Tuesday", 1, 0)

#testing w another regression w the flagts to see if "rly significant"
model_flags <- glm(D_or_L ~ UPS_flag + NYC_flag + Boston_flag + Tuesday_flag,
                   data = uglydf, family = binomial) 
summary(model_flags) #only Boston still significant!?


# Get 95% confidence intervals for these hoes. IF THE LEFT IS OVER 1 OR THE
#RIGHT IS UNDER 1 THEN ITS SIGNIFICANT
exp(confint(model_flags)) #NYC, Boston
exp(confint(AllReg)) #distance, transit days(duh), Tuesday, OnTrac, FedEx
#yes the results are different; no idk
```

Then I tried cross-validating it to see if I could get anything good out of it. I chose 5 folds to make it easy and went for it, using all variables.

```{r}
set.seed(2253221)
cv_ctrl = trainControl(method = "cv", number = 5)  # 5-fold CV

df$D_or_L <- as.factor(df$D_or_L)

cvmodel = train(
  D_or_L ~ Weight_kg + Distance_miles + Cost + Transit_Days +
    Origin_Warehouse + Destination + Carrier + ShipWeekday,
  data = df,
  method = "glm",
  family = "binomial",
  trControl = cv_ctrl
)

cvmodel
```

Accuracy and Kappa were both great, which was nice after the frustrating adventure so far. Realizing the model wasn't so bad, I ran a stepwise function to see if I could reduce the parameters and keep some accuracy.

```{r}
#| include: false
model_vars <- c("D_or_L", "Weight_kg", "Distance_miles", "Cost", 
                "Transit_Days", "Origin_Warehouse", "Destination", 
                "Carrier", "ShipWeekday") 

# Create a clean data frame
df_clean <- df[complete.cases(df[, model_vars]), ] 

### Re-fit your full model using the clean data frame
AllReg_clean <- glm(D_or_L ~ Weight_kg + Distance_miles + Cost + Transit_Days + 
                    Origin_Warehouse + Destination + Carrier + ShipWeekday, 
                    data = df_clean, family = binomial)
```

```{r}
stepmodel = stats::step(AllReg_clean, direction = "both") ##############stepwise
summary(stepmodel)
```

As the summary says, only two variables are needed in the model, Distance and Transit Days. Therefore, the improved model for predicting package delays and losses would be:

$$
DelayedOrLost = -3.2579491 + .4139328(TransitDays) - .0009309(DistanceMiles)
$$

Nice and succinct. However, there's a problem: of COURSE Transit Days is correlated with there being a delay or loss in package, because being in transit more days and being delayed; it's a data leakage/target leakage. So, I removed Transit Days from the dataset to try and determine what else might be a significant predictor:

```{r}
NoTransit = glm(
  D_or_L ~ Weight_kg + Distance_miles + Cost +
    Origin_Warehouse + Destination + Carrier + ShipWeekday,
  data = df,
  family = binomial
)

summary(NoTransit)
```

And ran a cross-validation, with necessary data-wrangling to make it work:

```{r}
set.seed(2253221)

NOTRANSIT.ctrl = trainControl(
  method = "cv",
  number = 8,       
  sampling = "up",     
  savePredictions = "final",
  classProbs = TRUE,   #lets me do other stuff idk
)

#aside stuff so the cv model works
uglydf$D_or_L <- as.character(df$D_or_L)
uglydf$D_or_L[df$D_or_L %in% c("0", 0)] <- "OnTime"
uglydf$D_or_L[df$D_or_L %in% c("1", 1)] <- "Delayed"

NOTRANSIT.cvmodel = train(
  D_or_L ~ Weight_kg + Distance_miles + Cost +
    Origin_Warehouse + Destination + Carrier + ShipWeekday,
  data = uglydf, #############################################
  method = "glm",
  family = "binomial",
  trControl = NOTRANSIT.ctrl
)

NOTRANSIT.cvmodel 
```

As you can see, the accuracy is *way* down compared to the model that included Transit Days. A meager 55% is looking very bleak. I ran the step-wise again to make sure my sinking suspicions were correct:

```{r}
stepNOTRANSIT = stats::step(NoTransit, direction = "both")
summary(stepNOTRANSIT) 
```

Yup. Not a single variable makes it to the final model—nothing is significant, and it's a wash. Therefore, regardless of the integrity of keeping data leakage in the model, that's our best bet we can achieve with logistic regression. So, let's have a hack at something else: LDA

## Linear Discriminant Analysis

I unfortunately had to run it manually since every single regular method wasn't working properly due to package issues. The code is 141 lines so I'm hiding it, but here's the output:

```{r}
#| code-fold: true
#| code-summary: "Show R Code"
library(MASS) # For lda()
library(pROC) # For roc()

# --- PASTE THIS CHUNK *BEFORE* YOUR CV LOOP ---

# 1. Dummy-code categoricals
dmy = dummyVars(~ Origin_Warehouse + Destination + Carrier + ShipWeekday,
                data = df, fullRank = TRUE)
Xcat = predict(dmy, newdata = df)

# 2. Assemble modeling frame
df_lda = cbind(
  D_or_L = df$D_or_L,
  Weight_kg = df$Weight_kg,
  Distance_miles = df$Distance_miles,
  Cost = df$Cost,
  Xcat
)
df_lda = as.data.frame(df_lda)
df_lda = na.omit(df_lda)

# 3. Normalize target labels for LDA
df_lda$D_or_L = as.character(df_lda$D_or_L)
df_lda$D_or_L[df_lda$D_or_L %in% c("2", 2, "Lost", "Delayed/Lost", "Delayed")] = "Delayed"
df_lda$D_or_L[df_lda$D_or_L %in% c("1", 1, "OnTime", "on time", "ontime")] = "OnTime"
df_lda$D_or_L = factor(df_lda$D_or_L, levels = c("Delayed", "OnTime"))

# 4. Assign prior probabilities
lda_prior = c(0.5, 0.5) 
names(lda_prior) = levels(df_lda$D_or_L) 

# --- END OF PREP CHUNK ---

set.seed(2253221)
k_folds <- 8

# Create fold assignments
folds <- cut(seq(1, nrow(df_lda)), breaks = k_folds, labels = FALSE)
# Randomly shuffle the assignments
folds <- sample(folds) 

# Create a data frame to store the metrics from each fold
# --- UPDATED to include Accuracy and Kappa ---
all_metrics <- data.frame(
  ROC = numeric(), 
  Sens = numeric(), 
  Spec = numeric(),
  Accuracy = numeric(),
  Kappa = numeric()
)

print("Starting manual 8-fold CV for LDA...")

# --- Start the CV loop ---
for (i in 1:k_folds) {
  
  # 1. Split data into training and testing
  test_indices <- which(folds == i)
  test_data <- df_lda[test_indices, ]
  train_data <- df_lda[-test_indices, ]

  # 2. Fit the LDA model directly (no caret)
  lda_fit <- MASS::lda(
    D_or_L ~ ., 
    data = train_data,
    prior = lda_prior 
  )
  
  # 3. Predict on the test data
  lda_pred <- predict(lda_fit, newdata = test_data)
  
  # --- Calculate Metrics Manually ---
  
  # (Skip fold if test set doesn't have both classes)
  if (length(unique(test_data$D_or_L)) < 2) {
    all_metrics[i, ] <- NA
    next 
  }

  # 1. ROC/AUC (from pROC)
  roc_obj <- roc(
    response = test_data$D_or_L,
    predictor = lda_pred$posterior[, "Delayed"], # Probability of "Delayed"
    levels = c("OnTime", "Delayed")
  )
  
  # 2. Confusion Matrix components
  conf_table <- table(
    Predicted = lda_pred$class, 
    Actual = test_data$D_or_L
  )
  
  # --- (Safeguard for 2x2 table) ---
  if (!("Delayed" %in% rownames(conf_table))) { conf_table <- rbind(conf_table, "Delayed" = c(0, 0)) }
  if (!("OnTime" %in% rownames(conf_table))) { conf_table <- rbind(conf_table, "OnTime" = c(0, 0)) }
  if (!("Delayed" %in% colnames(conf_table))) { conf_table <- cbind(conf_table, "Delayed" = c(0, 0)) }
  if (!("OnTime" %in% colnames(conf_table))) { conf_table <- cbind(conf_table, "OnTime" = c(0, 0)) }
  conf_table <- conf_table[c("Delayed", "OnTime"), c("Delayed", "OnTime")]
  # --- (End Safeguard) ---

  # Your positive class is "Delayed"
  TP <- conf_table["Delayed", "Delayed"] # True Positive
  FN <- conf_table["OnTime", "Delayed"]  # False Negative
  TN <- conf_table["OnTime", "OnTime"]   # True Negative
  FP <- conf_table["Delayed", "OnTime"]  # False Positive

  Sens <- TP / (TP + FN)
  Spec <- TN / (TN + FP)
  
  # --- NEW: Calculate Accuracy and Kappa ---
  
  Total <- TP + FN + TN + FP
  
  # Accuracy
  Accuracy <- (TP + TN) / Total
  
  # Kappa
  # P_o (observed agreement) is just Accuracy
  # P_e (expected agreement)
  prob_actual_delayed <- (TP + FN) / Total
  prob_actual_ontime  <- (FP + TN) / Total
  prob_pred_delayed   <- (TP + FP) / Total
  prob_pred_ontime    <- (FN + TN) / Total
  
  prob_e <- (prob_actual_delayed * prob_pred_delayed) + (prob_actual_ontime * prob_pred_ontime)
  
  Kappa <- (Accuracy - prob_e) / (1 - prob_e)
  
  # --- (End New) ---

  
  # Store all metrics for this fold
  all_metrics[i, ] <- c(as.numeric(roc_obj$auc), Sens, Spec, Accuracy, Kappa)
}
# --- End of loop ---

# 4. Get your final, averaged results
print("--- Manual CV Results ---")
print(colMeans(all_metrics, na.rm = TRUE))
```

All three results are pretty trash and unreliable, frankly. And this was AFTER upsampling. Terrible accuracy, and specificity and sensitivity are nothing at all to write home about. Not that that's too surprising—LDA, being classification, probably was never a good bet for a dataset like this where categories are so disparate in size, \>1600 to \<100.

Additionally, the LDA's low ROC score indicates that the relationship between the predictors (e.g., carrier, distance, cost) and a package delay is not simple or linear.

Instead of trying to play with the LDA model over and over to make it work, I moved on to something else, that proved more reliable: random forests.

## Random Forest

I had to clean up the data again to make it work, which is below. How it takes response variables is different from LDA so it needed changed again.

```{r}
df$D_or_L = as.character(df$D_or_L)
df$D_or_L[df$D_or_L %in% c("1", 1, "Delayed", "Lost", "Delayed/Lost")] = "Delayed"
df$D_or_L[df$D_or_L %in% c("0", 0, "OnTime", "on time", "ontime")] = "OnTime"
df$D_or_L = factor(df$D_or_L, levels = c("Delayed", "OnTime"))
```

Then I made a control and ran the tree:

```{r}
set.seed(2253221)
rf_ctrl = trainControl(
  method = "cv",
  number = 8,
  sampling = "up",
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final",
  verboseIter = TRUE
)

#train RF
rf_model = train(
  D_or_L ~ Weight_kg + Distance_miles + Cost + Transit_Days +
    Origin_Warehouse + Destination + Carrier + ShipWeekday,
  data = df,
  method = "rf",
  metric = "ROC",
  trControl = rf_ctrl,
  tuneLength = 5
)

rf_model
plot(rf_model)
```

I tuned the random forest model across 5 different values for the mtry hyperparameter, which controls the number of variables considered at each split. The 8-fold cross-validation determined that an mtry of 20 provided the optimal balance, given that it yielded the highest ROC score.

I also ran the tree, but measuring for accuracy/kappa instead of ROC/sens/spec:

```{r}
set.seed(2253221)
rf_ctrl = trainControl(
  method = "cv",
  number = 8,
  sampling = "up",
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final",
  verboseIter = TRUE
)

#train RF
rf_model = train(
  D_or_L ~ Weight_kg + Distance_miles + Cost + Transit_Days +
    Origin_Warehouse + Destination + Carrier + ShipWeekday,
  data = df,
  method = "rf",
  metric = "ROC",
  trControl = rf_ctrl,
  tuneLength = 5
)

rf_model
plot(rf_model)
```

The accuracy was .91 here, which isn't bad at all honestly. Others on Kaggle only managed .85 or so after making efforts with pruning to improve the confusion matrix, so I'm happy with my result.

```{r}
varImp(rf_model)
```

The ROC is much better at .85, and sensitivity is promising at .62. This means that of all the packages that were actually delayed, we successfully caught 62% of them. The model has a super high specificity of .98 as well, which means it's extremely accurate at not flagging on-time packages, with a 98% success rate. This ensures operational resources aren't wasted on false alarms.

Also, looking at variable importance we find that Transit Days is in the far lead, with the other three numeric variables (Distance, Cost, and Weight) lagging behind but still noticeable, and all of the categorical variables far from significant.

## Conclusion

Synthesizing the results of the three models, wherein with logistic regression stepwise we discovered Transit Days and Distance were only significant predictors, LDA where the whole thing was a wash, and random forest where Transit Days, Distance, Cost, and Weight all looked fairly significant, we can end with the consideration that Transit Days and Distance are the only two predictors we should really consider in a reduced-yet-still-efficient model.

Of course, there's still the problem with Transit Days being a target leakage, which frankly reduces the utility of this dataset into nothingness. Which, to be fair, makes sense—people work very hard to make sure packages come, and come quick, so that delays and losses isn't actually tied to anything makes sense. People working in supply chain and logistics would have already made sure to address an issue if there was one, meaning it makes sense that delay/loss in packages, when omitting the Transit Days variable, is up to random chance. In other words, preventing package delays seems to have already been fully optimized, at least according to the delay at hand.

This should, of course, be celebrated. Though it certainly made this analysis frustrating, the best thing you can do is find no problems or weak areas, which is exactly what happened.
